{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-classification-of-comments-using-BERT\" data-toc-modified-id=\"Sentiment-classification-of-comments-using-BERT-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sentiment classification of comments using BERT</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Description\" data-toc-modified-id=\"Data-Description-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data Description</a></span></li></ul></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Loading-Libraries\" data-toc-modified-id=\"Loading-Libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Loading Libraries</a></span></li><li><span><a href=\"#Loading-and-viewing-data\" data-toc-modified-id=\"Loading-and-viewing-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Loading and viewing data</a></span></li><li><span><a href=\"#Text-preprocessing-with-BERT\" data-toc-modified-id=\"Text-preprocessing-with-BERT-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Text preprocessing with BERT</a></span></li></ul></li><li><span><a href=\"#Models-training\" data-toc-modified-id=\"Models-training-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Models training</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#LogisticRegression-test\" data-toc-modified-id=\"LogisticRegression-test-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>LogisticRegression test</a></span></li><li><span><a href=\"#Comparing-Models-with-Standard-Hyperparameters\" data-toc-modified-id=\"Comparing-Models-with-Standard-Hyperparameters-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Comparing Models with Standard Hyperparameters</a></span></li><li><span><a href=\"#RandomForestClassifier\" data-toc-modified-id=\"RandomForestClassifier-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>RandomForestClassifier</a></span></li><li><span><a href=\"#SVC\" data-toc-modified-id=\"SVC-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>SVC</a></span></li><li><span><a href=\"#DummyClassifier\" data-toc-modified-id=\"DummyClassifier-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>DummyClassifier</a></span></li></ul></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Conclusions</a></span></li><li><span><a href=\"#Checklist\" data-toc-modified-id=\"Checklist-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Checklist</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment classification of comments using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key points:** BERT, text preprocessing, lemmatization, text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An online store needs a model for classifying comments into positive and negative.\n",
    "We have a dataset with markup about the mood of comments.\n",
    "\n",
    "**Task:** build a model with a quality metric value *F1* not less than 0.75.\n",
    "\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data is in the file `toxic_comments.csv`.  \n",
    "The *text* column contains the comment text, and *toxic* is the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "!pip install CatBoost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrey\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and viewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv('toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Distribution of \"normal\" and \"toxic\" comments'}, ylabel='Frequency'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data['toxic'].plot(kind='hist', title='Distribution of \"normal\" and \"toxic\" comments')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "- Comments need to be processed and translated into a form suitable for analysis\n",
    "- There is an imbalance in the target feature, we need to take it into account in further work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = data['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, \n",
    "                                                          truncation=True ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799e462a2fc7427b9ea5cd5b07fc5089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1595.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-85e805c81179>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     attention_mask[batch_size*i:batch_size*(i+1)])\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mbatch_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_embeddings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    956\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         )\n\u001b[1;32m--> 958\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    959\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    557\u001b[0m                 )\n\u001b[0;32m    558\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    560\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 495\u001b[1;33m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[0;32m    496\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m   1785\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1787\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1692\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1693\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n",
    "    attention_mask_batch = torch.LongTensor(\n",
    "    attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings1 = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(embeddings1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(embeddings)\n",
    "target = data['toxic']\n",
    "\n",
    "train_features, valid_features, train_target, valid_target = train_test_split(\n",
    "features, target, test_size=0.4, random_state = 42)\n",
    "\n",
    "valid_features, test_features, valid_target, test_target = train_test_split(\n",
    "valid_features, valid_target, test_size=0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_features.shape, train_target.shape)\n",
    "print(valid_features.shape, valid_target.shape)\n",
    "print(test_features.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_p1 = 0\n",
    "best_p2 = 0\n",
    "\n",
    "for p1 in [10000, 1000, 500, 350, 200, 100, 10, 1.0, 0.1, 0.01]:\n",
    "    for p2 in ['balanced', 'unbalanced']:\n",
    "            model =  LogisticRegression(max_iter = 10000, C=p1, random_state=42, class_weight=p2)\n",
    "            model.fit(train_features, train_target)\n",
    "            predicted_valid = model.predict(valid_features)\n",
    "            result = f1_score(predicted_valid, valid_target)\n",
    "            if result > best_result:\n",
    "                best_model = model\n",
    "                best_result = result\n",
    "                best_p1 = p1\n",
    "                best_p2 = p2\n",
    "                report = classification_report(valid_target, predicted_valid)\n",
    "                    \n",
    "print(\"valid F1:\", best_result, \"best parameter 1:\", best_p1, \", best parameter 2:\", best_p2)\n",
    "print()\n",
    "print(report)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter = 10000, C=best_p1, random_state=42, class_weight=best_p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_features, train_target)\n",
    "\n",
    "predictions_valid = model.predict(valid_features)\n",
    "predictions = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1 score valid:', f1_score(valid_target, predictions_valid))\n",
    "print('F1 score test:', f1_score(test_target, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Models with Standard Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "\n",
    "models_train,predictions_train = clf.fit(train_features, valid_features, train_target, valid_target)\n",
    "models_test,predictions_test = clf.fit(train_features, test_features, train_target, test_target)\n",
    "\n",
    "models_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.title('F1 score of test data for different models')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.barplot(y=models_test.index, x=\"F1 Score\", data=models_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_model = None\n",
    "best_result = 0\n",
    "best_p1 = 0\n",
    "best_p2 = 0\n",
    "\n",
    "for p1 in range(1, 50, 10):\n",
    "    for p2 in range (1, 100, 10):\n",
    "            model =  RandomForestClassifier(max_depth=p1, n_estimators=p2, random_state=42, class_weight='balanced')\n",
    "            model.fit(train_features, train_target)\n",
    "            predicted_valid = model.predict(valid_features)\n",
    "            result = f1_score(predicted_valid, valid_target)\n",
    "            if result > best_result:\n",
    "                best_model = model\n",
    "                best_result = result\n",
    "                best_p1 = p1\n",
    "                best_p2 = p2\n",
    "                    \n",
    "print(\"valid F1:\", best_result, \"best parameter 1:\", best_p1, \", best parameter 2:\", best_p2)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_depth=21, n_estimators=51, random_state=42, class_weight='balanced')\n",
    "model.fit(train_features, train_target)\n",
    "\n",
    "predictions = model.predict(test_features)\n",
    "\n",
    "print('F1 score test:', f1_score(test_target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(train_features, train_target)\n",
    "#Pipeline(steps=[('standardscaler', StandardScaler()), ('svc', SVC(gamma='auto'))])\n",
    "\n",
    "predictions_valid = clf.predict(valid_features)\n",
    "predictions = clf.predict(test_features)\n",
    "\n",
    "print('F1 score valid:', f1_score(valid_target, predictions_valid))\n",
    "print('F1 score test:', f1_score(test_target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(train_features, train_target)\n",
    "dummy_predictions = dummy_clf.predict(test_features)\n",
    "print('F1 score test:', f1_score(test_target, dummy_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_target, dummy_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The best result was shown by the logistic regression model and SVC - F1 score: 0.75 on test data\n",
    "2. For training the model, we selected only 4800 records out of almost 160 thousand - with the available computer power, processing all the data would take weeks.\n",
    "3. The BERT model did a good job and even the selected 4800 records allow for good model training.\n",
    "4. The considered models show the result better than the constant models\n",
    "5. For serious work, it is worth connecting additional. power and process all the data - this should increase the accuracy of the models even more.\n",
    "6. The indicators for the validation and test samples are approximately the same, which indicates a low probability that the models were overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Jupyter Notebook open\n",
    "- [x] All code is executed without errors\n",
    "- [x] The cells with the code are arranged in the order of execution\n",
    "- [x] Data loaded and prepared\n",
    "- [x] Models trained\n",
    "- [x] Metric value * F1 * not less than 0.75\n",
    "- [x] Conclusions written"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "179px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
